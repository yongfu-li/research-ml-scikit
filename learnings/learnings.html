<h1 id="supervised-learning">Supervised learning</h1>
<p>1.1. <a href="#Generalized-Linear-Models">Generalized Linear Models</a> 1.2. Linear and Quadratic Discriminant Analysis 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.5. Stochastic Gradient Descent 1.6. Nearest Neighbors 1.7. Gaussian Processes 1.8. Cross decomposition 1.9. Naive Bayes 1.10. Decision Trees 1.11. Ensemble methods 1.12. Multiclass and multilabel algorithms 1.13. Feature selection 1.14. Semi-Supervised 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) # Unsupervised learning 2.1. Gaussian mixture models 2.2. Manifold learning 2.3. Clustering 2.4. Biclustering 2.5. Decomposing signals in components (matrix factorization problems) 2.6. Covariance estimation 2.7. Novelty and Outlier Detection 2.8. Density Estimation 2.9. Neural network models (unsupervised)</p>
<h3 id="generalized-linear-models">Generalized Linear Models</h3>
<p>ssss</p>
<h2 id="gaussian-processes">Gaussian Processes</h2>
<ul>
<li>Gaussian Processes (GP) are a generic supervised learning method designed to solve regression and probabilistic classification problems. #### The advantages of Gaussian processes are:</li>
<li>The prediction interpolates the observations (at least for regular kernels).</li>
<li>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</li>
<li>Versatile: different kernels can be specified. Common kernels are provided, but it is also possible to specify custom kernels. #### The disadvantages of Gaussian processes include:</li>
<li>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</li>
<li>They lose efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens.</li>
</ul>
<h2 id="sklearn.svm-support-vector-machines-svms">sklearn.svm: Support vector machines (SVMs)</h2>
<ul>
<li>a set of supervised learning methods used for classification, regression and outliers detection.</li>
</ul>
<h4 id="the-advantages-of-support-vector-machines-are">The advantages of support vector machines are:</h4>
<ul>
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater than the number of samples.</li>
<li>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</li>
<li>Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels. ### The disadvantages of support vector machines include:</li>
<li>If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.</li>
<li>SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).</li>
</ul>
<h3 id="estimators">Estimators</h3>
<h4 id="a.-svm.linearsvc---linear-support-vector-classification.">a. svm.LinearSVC() - Linear Support Vector Classification.</h4>
<h4 id="b.-svm.linearsvrepsilon-tol-c-loss-linear-support-vector-regression.">b. svm.LinearSVR([epsilon, tol, C, loss, …]) Linear Support Vector Regression.</h4>
<h4 id="c.-svm.nusvc---nu-support-vector-classification.">c. svm.NuSVC() - Nu-Support Vector Classification.</h4>
<ul>
<li>decision_function(X) - Distance of the samples X to the separating hyperplane.</li>
<li>fit(X, y[, sample_weight]) - Fit the SVM model according to the given training data.</li>
<li>get_params([deep]) - Get parameters for this estimator.</li>
<li>predict(X) - Perform classification on samples in X.</li>
<li>score(X, y[, sample_weight]) - Returns the mean accuracy on the given test data and labels.</li>
<li>set_params(**params) - Set the parameters of this estimator. #### d. svm.NuSVR() - Nu Support Vector Regression.</li>
<li>fit(X, y[, sample_weight]) - Fit the SVM model according to the given training data.</li>
<li>get_params([deep]) - Get parameters for this estimator.</li>
<li>predict(X) - Perform regression on samples in X.</li>
<li>score(X, y[, sample_weight]) - Returns the coefficient of determination R^2 of the prediction.</li>
<li>set_params(**params) - Set the parameters of this estimator. #### e. svm.SVR([kernel, degree, gamma, coef0, tol, …]) Epsilon-Support Vector Regression.</li>
</ul>
<h4 id="f.-svm.oneclasssvmkernel-degree-gamma---unsupervised-outlier-detection.">f. svm.OneClassSVM([kernel, degree, gamma, …]) - Unsupervised Outlier Detection.</h4>
<ul>
<li>decision_function(X) - Distance of the samples X to the separating hyperplane.</li>
<li>fit(X, y[, sample_weight]) - Fit the SVM model according to the given training data.</li>
<li>get_params([deep]) - Get parameters for this estimator.</li>
<li>predict(X) - Perform classification on samples in X.</li>
<li>set_params(**params) - Set the parameters of this estimator.</li>
</ul>
<h4 id="svm.svcc-kernel-degree-gamma-coef0-c-support-vector-classification.">svm.SVC([C, kernel, degree, gamma, coef0, …]) C-Support Vector Classification.</h4>
<ul>
<li>svm.l1_min_c(X, y[, loss, fit_intercept, …]) Return the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty.</li>
</ul>
<h4 id="low-level-methods">Low-level methods</h4>
<ul>
<li>svm.libsvm.cross_validation Binding of the cross-validation routine (low-level routine)</li>
<li>svm.libsvm.decision_function Predict margin (libsvm name for this is predict_values)</li>
<li>svm.libsvm.fit Train the model using libsvm (low-level method)</li>
<li>svm.libsvm.predict Predict target values of X given a model (low-level method)</li>
<li>svm.libsvm.predict_proba Predict probabilities</li>
</ul>
<h4 id="examples">Examples</h4>
<pre><code># fit the model
clf = svm.NuSVC()
clf.fit(X, Y)
# plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])</code></pre>
<h2 id="skimage.color.adapt_rgb">skimage.color.adapt_rgb</h2>
<p><strong>adapt_rgb</strong></p>
<p><strong>each_channel</strong> - Pass each of the RGB channels to the filter one-by-one, and stitch the results back into an RGB image.</p>
<p>hsv_value</p>
